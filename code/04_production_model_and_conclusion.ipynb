{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd176c23-9bd0-4b41-a06b-d33866bec8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion\n",
    "# did we answer the problem statement/purpose\n",
    "# what can be improved in this process\n",
    "# what are the flaws of my model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e564b-6dba-4708-b699-4ace58f55774",
   "metadata": {},
   "source": [
    "# Production Model and Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedc6033-c743-4708-99de-195d30e27230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to remove\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f181daa-e699-464e-91b8-3238e1e550f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_to_num(data, feature):\n",
    "    '''\n",
    "    Takes in DataFrame and column name containing string ratings\n",
    "    replaces the ratings with numerical values\n",
    "    \n",
    "    Returns the updated DataFrame\n",
    "    '''\n",
    "    \n",
    "    # ratings that appear in the dataset\n",
    "    # and corresponding numerical value\n",
    "    qual_conversion = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0, np.nan: 0}\n",
    "    \n",
    "    # convert to ratings using dictionary\n",
    "    data[feature] = data[feature].apply(lambda x: qual_conversion[x])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def vif_df(df):\n",
    "    '''\n",
    "    Takes in a dataframe of numeric columns and computes\n",
    "    the Variance Inflation Factor (VIF) for each variable.\n",
    "    \n",
    "    Returns a nx1 DataFrame of VIF scores.\n",
    "    '''\n",
    "    \n",
    "    corr_features = df.columns\n",
    "    \n",
    "    # creats a list of VIF values, cycling through variables\n",
    "    # to select as a target for scoring\n",
    "    vif_values = [variance_inflation_factor(df.values, i) for i in range(len(corr_features))]\n",
    "    \n",
    "    # index scores with column names and sort from greatest to least\n",
    "    vif = pd.DataFrame(vif_values,\n",
    "                   index = corr_features,\n",
    "                   columns = ['vif']).sort_values(by = 'vif', ascending = False)\n",
    "    \n",
    "    return vif\n",
    "\n",
    "\n",
    "\n",
    "def setup_test():\n",
    "    '''\n",
    "    Generates a cleaned and feature engineered test dataset\n",
    "    that matches the general structure of the cleaned train data.\n",
    "    \n",
    "    Returns the cleaned, feature engineered test data set.\n",
    "    '''\n",
    "    \n",
    "    test_data = pd.read_csv('../datasets/test.csv')\n",
    "    \n",
    "    # initial features that were selected for the train dataset \n",
    "    test_features = ['MS Zoning', 'Lot Area', 'Land Contour', 'Land Slope', 'Neighborhood', 'Condition 1',\n",
    "            'Bldg Type', 'House Style', 'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add',\n",
    "           'Mas Vnr Type', 'Exter Qual', 'Exter Cond', 'Bsmt Qual', 'Bsmt Cond',\n",
    "           'Garage Area', 'Wood Deck SF', 'Open Porch SF', 'Kitchen Qual', 'Bedroom AbvGr',\n",
    "           'Full Bath', 'Half Bath', 'Fireplaces', 'Fireplace Qu', 'BsmtFin SF 1', 'Bsmt Unf SF', '1st Flr SF', '2nd Flr SF', 'Sale Type',]\n",
    "    \n",
    "    \n",
    "    test_data = test_data[test_features]\n",
    "    \n",
    "    # clean column names\n",
    "    test_data.columns = [col.lower().replace(' ','_').replace('/','_') for col in test_data.columns]\n",
    "    \n",
    "    \n",
    "    # converts descriptive rating to numerical\n",
    "    test_data = qual_to_num(test_data, 'bsmt_qual')\n",
    "    test_data = qual_to_num(test_data, 'bsmt_cond')\n",
    "    test_data = qual_to_num(test_data, 'kitchen_qual')\n",
    "    test_data = qual_to_num(test_data, 'exter_qual')\n",
    "    test_data = qual_to_num(test_data, 'exter_cond')\n",
    "    test_data = qual_to_num(test_data, 'fireplace_qu')\n",
    "    \n",
    "    # combines basment quality and condition ratings via multiplication\n",
    "    test_data['bsmt_qual_cond'] = test_data['bsmt_qual'] * test_data['bsmt_cond']\n",
    "    test_data.drop(columns = ['bsmt_qual', 'bsmt_cond'], inplace = True)\n",
    "    \n",
    "    test_data['qual_cond'] = test_data['overall_qual'] * test_data['overall_cond']\n",
    "    test_data.drop(columns = ['overall_qual', 'overall_cond'], inplace = True)\n",
    "\n",
    "    test_data['exter_qual_cond'] = test_data['exter_qual'] * test_data['exter_cond']\n",
    "    test_data.drop(columns = ['exter_qual', 'exter_cond'], inplace = True)\n",
    "\n",
    "    \n",
    "    # combines fireplace count and quality rating via multiplication\n",
    "    test_data['fireplaces_weighted'] = test_data['fireplaces'] * test_data['fireplace_qu']\n",
    "    test_data.drop(columns = ['fireplaces', 'fireplace_qu'], inplace = True)\n",
    "    \n",
    "    # adds square feet (SF) measurements with weights:\n",
    "    # 1*(Finished SF) + 0.5(Unfinished SF)\n",
    "    test_data['bsmt_weighted_sf'] = test_data['bsmtfin_sf_1'] + 0.5*test_data['bsmt_unf_sf']\n",
    "    test_data.drop(columns = ['bsmtfin_sf_1', 'bsmt_unf_sf'], inplace = True)\n",
    "    \n",
    "    # combines full bath and half bath into one column\n",
    "    test_data['bath'] = test_data['full_bath'] + 0.5*test_data['half_bath']\n",
    "    test_data.drop(columns = ['full_bath', 'half_bath'], inplace = True)\n",
    "    \n",
    "    #combines 1st floor and 2nd floor square feet areas\n",
    "    test_data['sq_ft'] = test_data['1st_flr_sf'] + test_data['2nd_flr_sf']\n",
    "    test_data.drop(columns = ['1st_flr_sf', '2nd_flr_sf'], inplace = True)\n",
    "    \n",
    "    # replace np.nan with most frequent value in the column 'None'\n",
    "    test_data.loc[test_data['mas_vnr_type'].isna(), 'mas_vnr_type'] = 'None'\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "def dummify_train_test(train, numeric_cols, categ_cols):\n",
    "    '''\n",
    "    Takes a DataFrame with the desired features to model with,\n",
    "    sets up a DataFrame of the test dataset with the same features,\n",
    "    get_dummies is performed on both DataFrames, and then makes sure\n",
    "    columns agree. If not, columns of zeros are added.\n",
    "    \n",
    "    Returns train and test DataFrames with desired features and\n",
    "    categorical columns dummified and in identical order.\n",
    "    '''\n",
    "    \n",
    "    # generate cleaned and feature engineered test dataframe\n",
    "    test = setup_test()\n",
    "    features = numeric_cols + categ_cols\n",
    "    \n",
    "    train_with_dummies = pd.get_dummies(train[features], columns = categ_cols, drop_first = True)\n",
    "    test_with_dummies = pd.get_dummies(test[features], columns = categ_cols, drop_first = True)\n",
    "    \n",
    "    # save dummy column names from both train and test\n",
    "    # to make sure both sets have identical column names/counts later\n",
    "    both_columns_set = set(train_with_dummies.columns).union(set(test_with_dummies.columns))\n",
    "\n",
    "    # for any column names not in set, add column with 0s\n",
    "    for col in both_columns_set:\n",
    "        if col not in train_with_dummies.columns:\n",
    "            train_with_dummies[col] = 0\n",
    "        if col not in test_with_dummies.columns:\n",
    "            test_with_dummies[col] = 0\n",
    "            \n",
    "    # make sure column order agrees for train and test sets\n",
    "    column_order = train_with_dummies.columns\n",
    "    test_with_dummies = test_with_dummies[column_order]\n",
    "    \n",
    "    return train_with_dummies, test_with_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973634e3-abe2-4a49-b1b5-07e15434b8cb",
   "metadata": {},
   "source": [
    "## Production Model Choice\n",
    "\n",
    "#### Rebuilding the Model\n",
    "Let us quickly rebuild our production linear regression model (Model 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2eb80b-e371-4025-9e91-fa7db760b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('../datasets/cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af60701-4e25-4534-983c-ffc36825223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = house['saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac162237-c3fb-4a43-b06a-0427f6653694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "numeric_features = ['sq_ft', 'garage_area', 'bsmt_weighted_sf', 'year_built']\n",
    "categ_features = ['neighborhood', 'house_style', 'kitchen_qual', 'qual_cond', 'exter_qual_cond']\n",
    "features = numeric_features + categ_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08291cfc-4c2d-49e2-8177-01bdbdec14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies on train and test datasets\n",
    "# and make columns agree for modeling\n",
    "train_dummy, test_dummy = dummify_train_test(house[features], numeric_features, categ_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a51cfd-317b-4215-b1e0-798eacabaf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_dummy, y, test_size = 0.3, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acce0697-bd55-4880-9546-992ebad0c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# generate prediction sale prices on our train (yt_pred) and validation (yv_pred)\n",
    "yt_pred = lr.predict(X_train)\n",
    "yv_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a77c57-d5b1-4514-a666-3cd1fa818c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cross Validation: 0.8904960587650373\n",
      "\n",
      "Train R squared: 0.914574607035238\n",
      "Train RMSE: 23340.119254589645\n",
      "\n",
      "Validation R squared: 0.887738294880876\n",
      "Validation RMSE: 25406.482714142392\n"
     ]
    }
   ],
   "source": [
    "# print scores for our train split\n",
    "print(f'Train Cross Validation: {cross_val_score(lr, X_train, y_train, cv = 5).mean()}\\n')\n",
    "print(f'Train R squared: {lr.score(X_train, y_train)}')\n",
    "print(f'Train RMSE: {metrics.mean_squared_error(y_train, yt_pred, squared = False)}\\n')\n",
    "\n",
    "# print scores for our validation split\n",
    "print(f'Validation R squared: {lr.score(X_val, y_val)}')\n",
    "print(f'Validation RMSE: {metrics.mean_squared_error(y_val, yv_pred, squared = False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5151d6a-46f3-4f8c-ab59-0de1e2f83f89",
   "metadata": {},
   "source": [
    "#### Justifying This Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e52f91-db2d-4de7-af3d-b2a958e16557",
   "metadata": {},
   "source": [
    "We see a decent improvement in our R squared scores and our RMSE scores are lower compared to our first three linear models. As expected our model performed a little worse on our validation set than on the training set, but the performance gain is still worth being slightly overfit.\n",
    "\n",
    "When we applied Ridge and Lasso models to the same features as the ones used here, our performance hardly improved. Because we did not have to scale our data to fit this model, we keep the interpretability of our coefficients. This is not the case for Ridge and Lasso. With the scores between the three models being so close, it makes the most sense to stick with our best linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadc657-55c5-40a4-af40-6a2aa6a26577",
   "metadata": {},
   "source": [
    "#### Model Inference\n",
    "\n",
    "Since coefficient interpretability is a perk of this model, let us examine them.\n",
    "\n",
    "To interpret the coefficients, we again need to identify which columns were dropped when we performed `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0daf7220-b144-43bd-be85-7094c97dc34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exter_qual_cond_2',\n",
       " 'house_style_1.5Fin',\n",
       " 'neighborhood_Blmngtn',\n",
       " 'qual_cond_1'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying columns get_dummies dropped\n",
    "no_drop_dummies = pd.get_dummies(house[features], columns = categ_features)\n",
    "set(no_drop_dummies.columns) - set(train_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd9dc5-3103-430f-aec2-e9da01d290a1",
   "metadata": {},
   "source": [
    "Our above results tell use that all of the coefficients are relative to a 1.5-story home in Bloomington Heights, with a overall quality/condition rating of 1 and exterior quality/condition rating of 2. These represent the baseline for all of our coefficients.\n",
    "\n",
    "Below, we see the coefficients which describe how much the price of a home with the baseline conditions changes if we increase the `year_built`, `sq_ft`, `bsmt_weighted_sf`, or the `garage_area` by one unit. We can see that a square foot of property is worth most in the main living space, least in the garage, and in between in the basement. We can also see the specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91201e1f-1e52-4e09-9fbd-9391553cfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of model coefficients with labels\n",
    "coef = pd.Series(lr.coef_, index = train_dummy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a59f18-aac1-4952-91b9-5c4c3164bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year_built          476.090864\n",
       "sq_ft                61.117261\n",
       "bsmt_weighted_sf     39.152619\n",
       "garage_area          31.278997\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted neighborhood coefficients\n",
    "coef[:4].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed25f95-0dba-4cc8-b850-50151f0a7ae5",
   "metadata": {},
   "source": [
    "Below is a list of that allows us to compare home values relative to Bloomington Heights. We can clearly see the most expensive neighborhood according to our model is Green Hills, while the least expensive is Meadow Village."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f54a677-da4b-4bf6-8d9e-f9172e4d8bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neighborhood_GrnHill    89919.264084\n",
       "neighborhood_StoneBr    44291.644347\n",
       "neighborhood_Veenker    32616.857581\n",
       "neighborhood_Crawfor    29326.138970\n",
       "neighborhood_NoRidge    25548.281974\n",
       "neighborhood_NridgHt    20132.195830\n",
       "neighborhood_ClearCr    16557.749918\n",
       "neighborhood_Timber     10934.985386\n",
       "neighborhood_Somerst    10464.234044\n",
       "neighborhood_SWISU       6629.850523\n",
       "neighborhood_Gilbert     6096.141013\n",
       "neighborhood_BrkSide     5956.337891\n",
       "neighborhood_CollgCr     4747.443224\n",
       "neighborhood_Sawyer       222.745616\n",
       "neighborhood_SawyerW      197.197578\n",
       "neighborhood_NAmes        -20.298748\n",
       "neighborhood_Mitchel     -643.214810\n",
       "neighborhood_IDOTRR     -2340.897408\n",
       "neighborhood_Edwards    -3007.094249\n",
       "neighborhood_NWAmes     -4436.321199\n",
       "neighborhood_OldTown    -5265.375505\n",
       "neighborhood_Blueste    -7130.563586\n",
       "neighborhood_Greens     -7411.014719\n",
       "neighborhood_NPkVill    -7843.510997\n",
       "neighborhood_BrDale     -8893.005381\n",
       "neighborhood_Landmrk   -11314.947598\n",
       "neighborhood_MeadowV   -18823.255041\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted neighborhood coefficients\n",
    "coef[coef.index.str.contains('neighborhood')].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005e032-024d-46c2-8b0d-38edb2e60d26",
   "metadata": {},
   "source": [
    "Next we can see that generally, more sizable homes had higher values. Weirdly, we see that homes with the second level unfinished seemed to be valued more than if it were finished. Maybe this is due to it being a blank slate and more readily designable to the new owners preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "369ee4eb-d53c-40b1-8212-ff947bc65dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "house_style_2.5Unf    17338.024491\n",
       "house_style_2.5Fin    12172.272016\n",
       "house_style_SLvl      10465.559449\n",
       "house_style_1.5Unf     9583.740654\n",
       "house_style_SFoyer     6584.137918\n",
       "house_style_1Story     6395.149133\n",
       "house_style_2Story      565.607490\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted house style coefficients\n",
    "coef[coef.index.str.contains('house_style')].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ddc3a-9bca-4d30-86d8-f833bd3f3d73",
   "metadata": {},
   "source": [
    "As for quality and condition, generally the higher the quality and condition score, the more expensive the home was (with some exceptions). Based on the coefficients, these quality and condition values produced some of the largest differences in home prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e936e6a-dfe0-4918-a81f-5124666977de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qual_cond_90    222660.792737\n",
       "qual_cond_50    103403.025428\n",
       "qual_cond_45     72950.171958\n",
       "qual_cond_72     46055.562486\n",
       "qual_cond_32     39487.132388\n",
       "qual_cond_63     39017.058976\n",
       "qual_cond_56     35911.573454\n",
       "qual_cond_49     35261.558927\n",
       "qual_cond_40     33177.678208\n",
       "qual_cond_64     30825.760368\n",
       "qual_cond_48     27834.320843\n",
       "qual_cond_42     18161.129597\n",
       "qual_cond_35     13952.388170\n",
       "qual_cond_28     12822.179763\n",
       "qual_cond_36     10904.106833\n",
       "qual_cond_30      4603.476192\n",
       "qual_cond_54      3188.259805\n",
       "qual_cond_24      2836.846742\n",
       "qual_cond_10      2308.225374\n",
       "qual_cond_60         0.000000\n",
       "qual_cond_20     -2052.282151\n",
       "qual_cond_25     -3019.688605\n",
       "qual_cond_9      -3268.595315\n",
       "qual_cond_12     -3707.833187\n",
       "qual_cond_8      -4634.262912\n",
       "qual_cond_16    -14161.504711\n",
       "qual_cond_18    -14597.007975\n",
       "qual_cond_6     -17716.266574\n",
       "qual_cond_15    -23536.940461\n",
       "qual_cond_21    -26918.287806\n",
       "qual_cond_4     -29321.762522\n",
       "qual_cond_5     -37841.113200\n",
       "qual_cond_3     -41801.912771\n",
       "qual_cond_27   -148926.853191\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted \n",
    "coef[(coef.index.str.contains('qual_cond')) &\n",
    "     (~coef.index.str.contains('exter'))].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad016587-766b-45fe-9c25-10af101453d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exter_qual_cond_15    39897.166078\n",
       "exter_qual_cond_16    23166.022635\n",
       "exter_qual_cond_20    22100.697779\n",
       "exter_qual_cond_12    18403.524164\n",
       "exter_qual_cond_9     17645.405940\n",
       "exter_qual_cond_6     12989.639271\n",
       "exter_qual_cond_4      9567.301388\n",
       "exter_qual_cond_3         0.000000\n",
       "exter_qual_cond_8     -6706.066320\n",
       "exter_qual_cond_25   -99222.577736\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted exterior quality/condition coefficients\n",
    "coef[coef.index.str.contains('exter')].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554b780-260c-405f-bac4-4d4846eea300",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a0350-f59a-4ad6-9a93-65fd4633ea54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798530e-04ff-4203-9118-b8d6f8ec2b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30307a-25f7-4dba-8688-ca8dde8ed893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e271524-78e3-4ca9-925b-e90189a96400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543bee3-27be-4337-8958-4cc48d1687ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
